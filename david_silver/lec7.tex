\section{Lecture 7 | Policy Gradient}
The main idea is to directly parametrise the policy
\[
\pi_\theta(a|s) = \mathbb{P}[a|s,\theta]  
\]
instead of derviing the policy from the value functions.

\subsubsection*{Advantages and Disadvantages of Policy Gradient}
\begin{itemize}
    \item \textbf{Advantages}
    \begin{itemize}
        \item Better convergence properties
        \item Effective in high-dimensional or continuous action spaces
        \item Can learn stochastic policies
    \end{itemize}
    \item \textbf{Disadvantages}
    \begin{itemize}
        \item Typically converge to a local rather than global optimum
        \item Evaluating a policy is typically inefficient and high variance
    \end{itemize}
\end{itemize}

\subsection{Policy Search}
The goal is to find the policy \(\pi _\theta (s,a) \) with parameters \(\theta\),
fnd the best parameters \(\theta\). The quality of a policy \(J(\theta)\) is defined by:

\begin{itemize}
    \item In episodic enviroments we can use the start value:
    \[
        J(\theta) = V^{\pi_\theta}(s_0) = \mathbb{E}_{\pi_\theta}[v_{\pi_\theta}(s_0)]  
    \]
    \item In continuing enviroments we can use the average value:
    \[
        J(\theta) = \sum_{s \in \mathcal{S}} d^{\pi_\theta}(s) V^{\pi_\theta}(s)
    \]
    \item we can also use the average reward per time-step:
    \[
        J(\theta) = \sum_{s \in \mathcal{S}} d^{\pi_\theta}(s) \sum_{a \in \mathcal{A}}
         \pi_\theta(a|s) \mathcal{R}^a_s
    \]
\end{itemize}
where \(d^{\pi_\theta}(s)\) is the stationary distribution of the Markov chain for \(\pi_\theta\).

Since the policy based reinforcement learning is an optimisation probelm we can use
the approaches from optimisation. We will use the gradient desdcent and its extensions 
to find the best parameters \(\theta\).

\subsection{Finite Difference Policy Gradient}
Let \(J(\theta)\) be any policy objective function. THe policy gradient algorthms seach for a local
maximum of \(J(\theta)\) by following the gradient of \(J(\theta)\) w.r.t. \(\theta\).
\[
    \Delta \theta = \alpha \nabla_\theta J(\theta)
\] 
where \(\nabla _\theta J(\theta)\) is the policy gradient and \(\alpha\) is the step size.
\[
    \nabla_\theta J(\theta) = \begin{bmatrix}
        \frac{\partial J(\theta)}{\partial \theta_1} \\
        \frac{\partial J(\theta)}{\partial \theta_2} \\
        \vdots \\
        \frac{\partial J(\theta)}{\partial \theta_n} \\
    \end{bmatrix}
\] 

\subsubsection{Computing the Policy Gradient by Finite Differences}
To evaluate the policy gradient of \(\pi _\theta (s,a) \) we can use the finite difference method.
\begin{itemize}
    \item For each dimension \(k \in [1,n]\) of the parameter vector \( \theta \) estimate
    the partial derivative of \(J(\theta)\) w.r.t. \(\theta_k\) by finite differences:
    \[
        \frac{\partial J(\theta)}{\partial \theta_k} \approx \frac{J(\theta + \epsilon \hat{k}) - J(\theta)}{\epsilon}
    \]
    where \(\hat{k}\) is a unit vector in the direction of the \(k\)-th axis.
\end{itemize}
This method uses \(n\) evaluations of the policy \( \pi _\theta (s,a) \) to estimate the policy gradient,
where \(n\) is the number of parameters in \(\theta\). This method is very inefficient but simple to implement,
and works for arbitrary policies, even if the policies are not differentiable.

\subsection{Monte-Carlo Policy Gradient (REINFORCE)}
Here we calculate the policy gradient analytically. Assume that the policy is differentiable
wherever it is non-zero, and we can compute the gradient \( \nabla_\theta \pi_\theta(s,a) \).

We are going to use the concept called likelihood ratio. Let \(p(x)\) and \(q(x)\) be two probability
distributions over the same random variable \(x\). The likelihood ratio is defined as:
\[
    L(x) = \frac{p(x)}{q(x)}
\]
The likelihood ratio is useful because it is the gradient of the log probability:
\[
    \nabla_x \log p(x) = \frac{\nabla_x p(x)}{p(x)} = \frac{p(x)}{p(x)} \frac{\nabla_x p(x)}{p(x)} = \frac{\nabla_x p(x)}{p(x)} = \nabla_x \log p(x)
\]
Thus, applying the likelihood ratio trick to the policy gradient:
\[
    \begin{aligned}
        \nabla _\theta  \pi _\theta (s,a) &= \pi _\theta (s,a) \frac{\nabla _\theta  \pi _\theta (s,a)}
        {\pi _\theta (s,a)} \\&= \pi _\theta (s,a) \nabla _\theta \log \pi _\theta (s,a)
    \end{aligned}
\]
Thus from the literature of statistics we define the score function as:
\[
    \nabla _\theta \log \pi _\theta (s,a)
\]
\begin{example}[Softmax Policy]
    The softmax policy weights the action using linear combinmation of features \(\phi (s,a)^{\top} \theta\),
    where the probability of taking action is proportional to the exponentiated weight:
    \[
        \pi _\theta (s,a) \propto e^{\phi (s,a)^{\top} \theta}
    \]
    Then the score function is defined as:
    \[
        \nabla _\theta \log \pi _\theta (s,a) = \phi (s,a) - \mathbb{E}_{\pi_\theta}[\phi (s,\cdot)]  
    \]
    The derivation of the above is:
    \[
        \begin{aligned}
            \pi_\theta(s,a) &= \frac{e^{\phi(s,a)^{\top} \theta}}{\sum\limits_{b \in \mathcal{A}} 
            e^{\phi(s,b)^{\top} \theta}} \\
            \implies \nabla_\theta \log \pi_\theta(s,a) &= \nabla_\theta \log e^{\phi(s,a)^{\top} \theta} -
            \nabla_\theta \log\left(   
             \sum\limits_{b \in \mathcal{A}} e^{\phi(s,b)^{\top} \theta} \right)\\
                &= \nabla _\theta \left( 
                    \phi(s,a)^{\top} \theta 
                 \right) - \frac{\nabla_\theta \left( 
                    \sum\limits_{b \in \mathcal{A}} e^{\phi(s,b)^{\top} \theta}
                    \right)}{\sum\limits_{b \in \mathcal{A}} e^{\phi(s,b)^{\top} \theta}} \\
                &= \phi(s,a) - \frac{\sum\limits_{b \in \mathcal{A}} \phi(s,b) e^{\phi(s,b)^{\top} \theta}}
                {\sum\limits_{b \in \mathcal{A}} e^{\phi(s,b)^{\top} \theta}} \\
                &= \phi(s,a) - \sum\limits_{b \in \mathcal{A}} \pi_\theta(s,b) \phi(s,b) \\
        \end{aligned}
    \]
    which is the difference between the feature vector of the action and the expected feature vector
\end{example}

\begin{example}[Gaussian Policy]
    In continous action spaces, a gaussian policy is often used. The mean of the gaussian is given by
    a linear combination of features \(\mu(s) = \phi(s)^{\top} \theta\), and the variance is fixed or
    can be parametrised as well. The probability of taking action \(a\) is given by \(a \sim 
    \mathcal{N}(\mu(s),\sigma^2)\). Then the score function is defined as:  
    \[
        \nabla _\theta \log \pi _\theta (s,a) = \frac{(a - \mu(s)) \phi(s)}{\sigma^2}
    \]
\end{example}

\subsubsection*{One Step MDPs}
Consider a simple calss of one step MDPs i.e. starting in the state \(s \sim d(s)\),
and terminating after one timestep with reward \(r = R(s,a)\). 

Using likelihood ration to compute the policy gradienbt we have:
\[
    \begin{aligned}
        J(\theta) &= \mathbb{E} _{\pi_\theta}[r] \\
        &= \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi_\theta(a|s) \mathcal{R}(s,a) \\
        \nabla_\theta J(\theta) &= \sum_{s \in \mathcal{S}} d(s) 
        \sum_{a \in \mathcal{A}} \pi _\theta (a|s) \nabla_\theta \log \pi _\theta (a|s) \mathcal{R}(s,a) \\
        &= \mathbb{E}_{\pi_\theta}
        \left[   
        \nabla_\theta \log \pi_\theta(a|s) r\right]
    \end{aligned}
\]
To extend this to multi-step MDPs we can use the policy gradient theorem. The policy grafient theorem
generalises the likelihood ratio trick to multi-step MDPs. It
replaces the instantaneous reward \(r\) with the long-term value \(Q^{\pi_\theta}(s,a)\).
\begin{theorem}[Policy Gradient Theorem]
    For any differentiable policy \(\pi_\theta(s,a)\), and for any of the policy objective
    functions \(J(\theta)\) defined above, the policy gradient is given by:
    \[
        \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ 
            \nabla_\theta \log \pi_\theta(a|s) Q^{\pi_\theta}(s,a)
        \right]
    \]
\end{theorem}

\subsubsection{REINFORCE Algorithm}
The REINFORCE algorith updates the policy parameters by stochastic gradient ascent on the policy,
using return \(v_t\) as an unbiased sample of \(Q^{\pi_\theta}(s,a)\).
\[
    \Delta \theta = \alpha \nabla_\theta \log \pi_\theta(a|s) v_t
\] 
Thus, the algorithm for the REINFORCE is given in \autoref{alg:reinforce}.
\begin{algorithm}[H]
    \caption{REINFORCE}
    \label{alg:reinforce}
    \begin{algorithmic}[1]
        \State Initialise policy parameters \(\theta\)
        \For{each episode}
            \State Generate an episode \(s_0,a_0,r_1,\dots,s_{T-1},a_{T-1},r_T\)
            \For{\(t = 0,1,\dots,T-1\)}
                \State \(G \leftarrow\) return from timestep \(t\)
                \State \(\theta \leftarrow \theta + \alpha G \nabla_\theta \log \pi_\theta(a_t|s_t)\)
            \EndFor
        \EndFor
    \end{algorithmic}
\end{algorithm}

\subsection{Actor Critic Policy Gradient}
Monte-Carlo policy gradient methods suffer from high variance. The variance can be reduced by
using a critic to estimate the action-value function:
\[
    Q_w(s,a) \approx Q^{\pi_\theta}(s,a)  
\]
Actor critic methods main two set of parameters: the actor parameters \(\theta\) that are updated
in the direction suggested by the policy gradient, and the critic parameters \(w\) that are updated
by policy evalution.
Thus, actor critic algorithms floow an approximate policy gradient:
\[
    \nabla_\theta J(\theta) \approx \mathbb{E}_{\pi_\theta} \left[ 
        \nabla_\theta \log \pi_\theta(a|s) Q_w(s,a)
    \right]
\]
\[
    \Delta \theta = \alpha \nabla_\theta \log \pi_\theta(a|s) Q_w(s,a)  
\]
Since crtic is just policy evalution, we can use the tools from the policy evalution to estimate
the action-value function. An algorith using linear function value approximation as the function
approximator is given in \autoref{alg:actor-critic}. This implies that critic updates \(\mathbf{w}\)
with linear TD, and the actor updates \(\theta\) with policy gradient.
\begin{algorithm}[H]
    \caption{Actor Critic with Linear Function Approximation}
    \label{alg:actor-critic}
    \begin{algorithmic}[1]
        \State Initialise policy parameters \(\theta\) and value function parameters \(\mathbf{w}\)
        \For{each episode}
            \State Initialise \(s\)
            \Repeat
                \State \(a \sim \pi_\theta(\cdot|s)\)
                \State Take action \(a\), observe \(r,s'\)
                \State \(\delta \leftarrow r + \gamma \mathbf{w}^{\top} \phi(s') - \mathbf{w}^{\top} \phi(s)\)
                \State \(\mathbf{w} \leftarrow \mathbf{w} + \alpha \delta \nabla_\mathbf{w} \phi(s)\)
                \State \(\theta \leftarrow \theta + \beta \delta \nabla_\theta \log \pi_\theta(a|s)\)
                \State \(s \leftarrow s'\)
            \Until{terminal}
        \EndFor
    \end{algorithmic}
\end{algorithm} 

\subsubsection{Bias in Actor Critic Algorithms}
Approximimating the policy gradient introduces bias in the policy gradient estimate. The bias
can be reduced by using a compatible function approximator. A function approximator is compatible
if the policy gradient is a gradient of the performance measure w.r.t. the parameters of the
function approximator.

\begin{theorem}[Compatible Function Approximator Theorem]
If the followinf conditions are satisfied:
\begin{itemize}
    \item Value functijn approximator is compatible with the policy:
    \[
        \nabla_w Q_w(s,a) = \nabla_\theta \log \pi_\theta(a|s)        
    \]
    \item The value function paramters \(w\) minimise the mean-squared error:
    \[
        \mathbb{E}_{\pi_\theta} \left[ 
            \left( Q_w(s,a) - Q_w(s,a) \right)^2
        \right]
    \]
\end{itemize}
Then the policy gradient is exact:
\[
    \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ 
        \nabla_\theta \log \pi_\theta(a|s) Q_w(s,a)
    \right]
\]
\end{theorem}
\begin{proof}
    If \(w\) is choosen to minimise the mean-squared error, then the gradient of the mean-squared
    wrt \(w\) is zero:
    \[
        \begin{aligned}
            0 &= \nabla _w \epsilon \\
            &= \mathbb{E} _{\pi_\theta} \left[ 
                (Q^{\pi_\theta}(s,a) - Q_w(s,a)) \nabla_w Q_w(s,a)
            \right] \\
            &= \mathbb{E} _{\pi_\theta} \left[ 
                (Q^{\pi_\theta}(s,a) - Q_w(s,a)) \nabla_\theta \log \pi_\theta(a|s)
            \right] \\
        \end{aligned}
    \]
    \[
        \implies \mathbb{E} _{\pi_\theta} \left[ 
            Q^{\pi_\theta}(s,a) \nabla_\theta \log \pi_\theta(a|s)
        \right] = \mathbb{E} _{\pi_\theta} \left[
            Q_w(s,a) \nabla_\theta \log \pi_\theta(a|s)
        \right]
    \]
    Thus, the policy gradient is exact. and can be substituted directly into the policy gradient
    \[
        \nabla J(\theta) = \mathbb{E} _{\pi_\theta} \left[ 
            Q_w(s,a) \nabla_\theta \log \pi_\theta(a|s)
        \right]
    \]
\end{proof}


\subsubsection{Reducing the Variance using a Baseline}
We can subtract a baseline function \(B(s)\) from the policy gradient withou changing the direction 
of the gradient ascent. In other words, the baseline function does not change the expected value
of the policy gradient.
\[
    \begin{aligned}
        \mathbb{E}_{\pi_\theta} \left[ 
            \nabla_\theta \log \pi_\theta(a|s) B(s)
        \right] &= \sum_{s \in \mathcal{S}} d^{\pi_\theta}(s) \sum_{a \in \mathcal{A}}
        \nabla_\theta \pi_\theta(a|s) B(s) \\
        &= \sum_{s \in \mathcal{S}} d^{\pi_\theta}(s) B(s) \sum_{a \in \mathcal{A}}
        \nabla_\theta \pi_\theta(a|s) \\
        &= \sum_{s \in \mathcal{S}} d^{\pi_\theta}(s) B(s) \nabla_\theta \sum_{a \in \mathcal{A}}
        \pi_\theta(a|s) \\
        &= \sum_{s \in \mathcal{S}} d^{\pi_\theta}(s) B(s) \nabla_\theta 1 \\
        &= 0
    \end{aligned}
\]
\begin{note}
    Refer this link for an extra informal proof: 
    \href{https://ai.stackexchange.com/questions/28568/why-adding-a-baseline-
    doesnt-affect-the-policy-gradient}{StackExchange}
\end{note}

A good baseline finction is the state value function \(B(s) = V^{\pi_\theta}(s)\). 
Thus, the policy gradient can be rewritten using the advantage function \(A^{\pi_\theta}(s,a)\):
\[
    \begin{aligned}
        A^{\pi_\theta}(s,a) &= Q^{\pi_\theta}(s,a) - V^{\pi_\theta}(s)\\
        \nabla_\theta J(\theta) &= \mathbb{E}_{\pi_\theta} \left[ 
            \nabla_\theta \log \pi_\theta(a|s) A^{\pi_\theta}(s,a)
        \right]
    \end{aligned}
\]
Intuitively, the advantage function measures how much better an action is than the average action.

\subsubsection{Estimating the Advantage Function}
The advantage function can significantly reduce the variance of the policy gradient. So the
critic can be used to estimate the advantage function. One way to estimate the advantage function
is to estimate the action-value function and the state-value function, and then compute the difference.
\[
\begin{aligned}
    V_v(s) &\approx V^{\pi_\theta}(s) \\
    Q_w(s,a) &\approx Q^{\pi_\theta}(s,a) \\
    A(s,a) &= Q_w(s,a) - V_v(s)
\end{aligned}
\]
Both set of parameters \(v\) and \(w\) can be updated by using say TD(0) learning.

Another way of estimating the advantage function is to use the TD error. The 
main idea is that for the true value fumction \(V^{\pi_\theta}(s)\) the TD error \(\delta ^{\pi_\theta}\)
is an unbiased estimate of the advantage function \(A^{\pi_\theta}(s,a)\).
\[
    \begin{aligned}
        \delta ^{\pi_\theta} &= r + \gamma V^{\pi_\theta}(s') - V^{\pi_\theta}(s) \\
        \implies \mathbb{E}_{\pi_\theta} \left[ 
            \delta ^{\pi_\theta} | s,a
        \right] &= \mathbb{E}_{\pi_\theta} \left[ 
            r + \gamma V^{\pi_\theta}(s') - V^{\pi_\theta}(s) | s,a
        \right] \\
        &= Q^{\pi_\theta}(s,a) - V^{\pi_\theta}(s) \\
        &= A^{\pi_\theta}(s,a)
    \end{aligned}
\]
Thus, allowing the use of TD error to compute the policy gradient:
\[
    \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ 
        \nabla_\theta \log \pi_\theta(a|s) \delta ^{\pi_\theta}
    \right]
\]
In practive we can use an approximate TD error \(\delta _v\) to estimate the advantage function:
\[
    \delta _v = r + \gamma V_v(s') - V_v(s)
\]
