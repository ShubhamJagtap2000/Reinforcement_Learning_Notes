\section{{Lecture 2 | Markov Decision Processes}}
\subsection{Markov Processes}
Markov decision process formally describe an enviroment for reinforcement
learning. The nice case for this setting is that the environemtn is fully observable. Thus,
the current state completely characterizes the process. This is called the Markov property.

Thus, all RL problems can bve formalised in terms of MDPs. Optimal Control problem can be 
formalised as continuous MDPs. Partially observable problems can be always converted 
to MDPs.
\subsubsection*{State Transition Matrix}
For a markov state \(s\) and succesor state \(s^{\prime} \) the state transition probability
is defined as:
\begin{equation}
    \mathcal{P} _{ss^{\prime}} = \mathcal{P} [S_{t+1} = s^{\prime} | S_{t} = s]
\end{equation}
Thus each row of the matrix is a probability distribution over the next state assuming
that we are in the state of the row. 
\[
    \mathcal{P} = 
    \begin{bmatrix}
        \mathcal{P} _{11} & \mathcal{P} _{12} & \dots & \mathcal{P} _{1n} \\
        \mathcal{P} _{21} & \mathcal{P} _{22} & \dots & \mathcal{P} _{2n} \\
        \vdots & \vdots & \ddots & \vdots \\
        \mathcal{P} _{n1} & \mathcal{P} _{n2} & \dots & \mathcal{P} _{nn} \\
    \end{bmatrix}  
\]
Each row sums over to 1.

THus, a markov process is a memoeryless random process, i.e. a sequence of random states
\(S_{1}, S_{2}, \dots \) with the Markov property. 

\begin{definition}[Markov Process]
    A Markov process is a tuple \((\mathcal{S}, \mathcal{P} )\) consisting of a finite set
    of states \(\mathcal{S} \) and a state transition probability matrix \(\mathcal{P} \).
    where,
    \[
        \mathcal{P} _{ss^{\prime}} = \mathbb{P}  [S_{t+1} = s^{\prime} | S_{t} = s]  
    \]
    for all \(s, s^{\prime} \in \mathcal{S} \).
\end{definition}
Random Process is a random sequence that is drawn from a probability distribution over 
the sequences of state given by the markov chain.

\subsection{Markov Reward Processes}
A Markov Reward Process is a Markov chain with values.
\begin{definition}[Markov Reward Process]
    A Markov Reward Process is a tuple \((\mathcal{S}, \mathcal{P} , \mathcal{R} , \gamma)\)
    consisting of:
    \begin{itemize}
        \item a finite set of states \(\mathcal{S} \)
        \item a state transition probability matrix \(\mathcal{P} \)
        \[
            \mathcal{P} _{ss^{\prime}} = \mathbb{P}  [S_{t+1} = s^{\prime} | S_{t} = s]  
        \]
        \item a reward function \(\mathcal{R} \)
        \[
            \mathcal{R} _{s} = \mathbb{E}  [R_{t+1} | S_{t} = s]  
        \]
        \item a discount factor \(\gamma \in [0, 1]\)
    \end{itemize}
\end{definition}
\begin{definition}[Return]
    The return \(G_{t}\) is the total discounted reward from time-step \(t\).
    \[
        G_{t} = R_{t+1} + \gamma R_{t+2} + \dots = \sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}  
    \]
\end{definition}
NBOTE: the goial of RL is to maximise the expected return from the start state. 

The discount factor \(\gamma \) determines the present value of future rewards. THe discount
factor of 0 makes the agent myopic, it only cares about immediate rewards. The discount 
factor of 1 makes the agent strive for a long-term reward.

\subsubsection*{Why do we use discounting?}
Most Markov reward processes and decision process are discounted. This is done so account 
for the uncertainity in the dynamics of the environement. It also allows us to converge to
a solution in the infinite/cyclic Markov processes. Somtimes it is possible to 
use undiscounted Markov processes, if all sequences terminate in a finite number of steps.

\subsubsection{Value Function}
The value function \(v(s)\) gives the long-term value of state \(s\). It is the expected 
return starting from state \(s\).
\begin{definition}[Value Function]
    The value function \(v(s)\) of an MRP is the expected return starting from state \(s\).
    \[
        v(s) = \mathbb{E}  [ G_{t} \ |\  S_{t} = s]  
    \]
\end{definition}

\subsubsection{Bellman Equation for MRPs}
The value function can be decomposed into two parts:
\begin{itemize}
    \item immediate reward \(R_{t+1}\)
    \item discounted value of successor state \(\gamma v(S_{t+1})\)
\end{itemize}
Thus we have:
\[
    \begin{aligned}
        v(s) & = \mathbb{E}  [ G_{t} \ |\  S_{t} = s] \\
             & = \mathbb{E}  [ R_{t+1} + \gamma R_{t+2} + \gamma^{2} R_{t+3} + \dots \ |\  S_{t} = s] \\
             & = \mathbb{E}  [ R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \ |\  S_{t} = s] \\
             & = \mathbb{E}  [ R_{t+1} + \gamma G_{t+1} \ |\  S_{t} = s] \\
             & = \mathbb{E}  [ R_{t+1} + \gamma v(S_{t+1}) \ |\  S_{t} = s] \quad \dots \text{using the law of iterated expectations} \\ 
    \end{aligned}
\]
\[
    \implies v(s) = \mathcal{R} _{s} + \gamma \sum_{s^{\prime} \in \mathcal{S} } \mathcal{P} _{ss^{\prime}} v(s^{\prime})
\]
Thus, the Bellman equation can be expreseed as a linear system of equations.
\[
    \begin{aligned}
        v &= \mathcal{R} + \gamma \mathcal{P} v  \\
        \begin{bmatrix}
             v(1) \\
             v(2) \\
             \vdots \\
             v(3) \\
        \end{bmatrix} & = 
        \begin{bmatrix}
            \mathcal{R} _{1} \\
            \mathcal{R} _{2} \\
            \vdots \\
            \mathcal{R} _{n} \\
        \end{bmatrix} + \gamma
        \begin{bmatrix}
            \mathcal{P} _{11} & \mathcal{P} _{12} & \dots & \mathcal{P} _{1n} \\
            \mathcal{P} _{21} & \mathcal{P} _{22} & \dots & \mathcal{P} _{2n} \\
            \vdots & \vdots & \ddots & \vdots \\
            \mathcal{P} _{n1} & \mathcal{P} _{n2} & \dots & \mathcal{P} _{nn} \\
        \end{bmatrix}
        \begin{bmatrix}
            v(1) \\
            v(2) \\
            \vdots \\
            v(3) \\
        \end{bmatrix} \\
        \end{aligned}
\]
SInce this is a linear system of equations, we can solve it using linear algebra.
\[
    \begin{aligned}
        v &= \mathcal{R} + \gamma \mathcal{P} v  \\
        (I - \gamma \mathcal{P} ) v &= \mathcal{R}  \\
        v &= (I - \gamma \mathcal{P} )^{-1} \mathcal{R}  \\      
    \end{aligned}
\]
Too large to compute in practice. Thus, we use iterative methods to solve
this equation.

\subsection{Markov Decision Processes}
A Markov Decision Process is a Markov Reward Process with decisions.
It is an environment in which all states are Markov. Thus the next state
that the MDp transitions to depends on the current state and the action
that the agent takes in the current state.

\begin{definition}[Markov Decision Process]
    A Markov Decision Process is a tuple \((\mathcal{S}, \mathcal{A} , \mathcal{P} , \mathcal{R} , \gamma)\)
    consisting of:
    \begin{itemize}
        \item a finite set of states \(\mathcal{S} \)
        \item a finite set of actions \(\mathcal{A} \)
        \item a state transition probability matrix \(\mathcal{P} \)
        \[
            \mathcal{P} _{ss^{\prime}}^{a} = \mathbb{P}  [S_{t+1} = s^{\prime} | S_{t} = s, A_{t} = a]  
        \]
        \item a reward function \(\mathcal{R} \)
        \[
            \mathcal{R} _{s}^{a} = \mathbb{E}  [R_{t+1} | S_{t} = s, A_{t} = a]  
        \]
        \item a discount factor \(\gamma \in [0, 1]\)
    \end{itemize}
\end{definition}
\subsubsection{Policy}
Formalises what it means to take decisions. 
\begin{definition}[Policy]
    A policy \(\pi \) is a distribution over actions given states.
    \[
        \pi (a|s) = \mathbb{P}  [A_{t} = a | S_{t} = s]  
    \]
\end{definition}
The policies fully define the behaviour of the agent. Usually,
the policies are stationary, i.e. they do not change over time.
The policies are dependent only on the current state and not on the
history of the agent.

NOTE: 
\begin{itemize}
    \item We can always recover the MRP or a Markov Process from an MDP, given
    and MDP \(\mathcal{M} = \langle \mathcal{S}, \mathcal{A} , 
    \mathcal{P} , \mathcal{R} , \gamma \rangle
    \)
    and a policy \(\pi\)
    \item If we have an policy and we sample the states using the policy,
    the state sequence is a Markov Process \( \langle \mathcal{S}, \mathcal{P} ^{\pi} \rangle \)
    
    \item Similarly, the state and reward sequence is a Markov Reward Process \( 
        \langle \mathcal{S}, \mathcal{P} ^{\pi}, \mathcal{R} ^{\pi}, \gamma \rangle \) 
\end{itemize}
